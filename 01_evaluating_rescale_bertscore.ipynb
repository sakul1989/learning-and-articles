{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watch out with leveraging BERTScore for the Evaluation of Language Models\n",
    "\n",
    "This is the notebook to my medium article: https://medium.com/@lukasheller1989/watch-out-with-leveraging-bertscore-for-the-evaluation-of-language-models-ed28dc365435\n",
    "\n",
    "\n",
    "As a natural language processing enthusiast, I recently embarked on a project where I delved into the realm of language model evaluation. In this journey, I stumbled upon an invaluable tool called BERTScore, which significantly enhanced the assessment process of my Language Model (LM). However, along the way, I encountered an intriguing phenomenon: the importance of setting rescale_with_baseline to True for accurate results. In this article, I‚Äôll share my insights on the significance of BERTScore for LM evaluation, the implications of its parameters, and why setting rescale_with_baseline to True is crucial.\n",
    "\n",
    "## 1. Introduction to BERTScore\n",
    "BERTScore is a metric designed to evaluate the quality of text generated by language models. Leveraging contextual embeddings from BERT (Bidirectional Encoder Representations from Transformers), it computes the similarity between a reference sentence and a generated sentence. Unlike traditional evaluation methods such as BLEU or ROUGE, BERTScore considers contextual information, leading to more accurate and human-like evaluations. That it is why it has become typical metric for evaluating the performance of Language models.\n",
    "\n",
    "## 2. Understanding BERTScore for Language Models\n",
    "BERTScore operates by computing the similarity between token-level representations of the reference and candidate sentences. By comparing the embeddings in context, it captures nuances that other metrics might miss. This makes it particularly well-suited for evaluating the performance of language models, which strive to generate coherent and contextually relevant text.\n",
    "\n",
    "## 3. The problem with BERTScore\n",
    "In my project I used Llama-7b and prompt engineering to extract TRIZ-concepts from patents. I did have annotated data. So I decided to evaluate my performance using BLEU, ROUGE and BERTScore. \n",
    "I had very different results for BLEU, ROUGE and BERTScore. Regarding BLEU and ROUGE Llama-7b performed rather poorly. However, looking at the BERTscore it seemed to be doing a more than okay job. I explained myself the results with different nature of the metrics. As BERTScore is far less literal and a lot more focused on true semantic similarity than traditional metrics like BLEU and ROUGE it can produce different results. I even wrote an example calculation to explain this phenomenon. For that consider the following sentence pairs which could be from a patent:\n",
    "\n",
    "### 1. Literal overlap:\n",
    "\n",
    "Candidate: ‚ÄúThe method comprises receiving an input signal from a sensor.‚Äù\n",
    "Reference: ‚ÄúThe method is about a sensor that recieves an input signal.‚Äù\n",
    "Both sentences convey similar information with overlapping n-grams, which should result in a high BLEU or ROUGE score due to the shared tokens.\n",
    "\n",
    "### 2. Semantic overlap:\n",
    "\n",
    "Candidate: ‚ÄúA computer is configured to communicate with a network for data transmission.‚Äù\n",
    "Reference: ‚ÄúData transfer is facilitated by a network-connectable laptop.‚Äù\n",
    "While the sentences express the same idea, there are no overlapping n-grams. Instead, they use different phrasings and synonyms, which would likely yield a lower BLEU or ROUGE score despite the semantic similarity. As BERTScore focues on semantic similarity it should also give a good result for this pair.\n",
    "\n",
    "### 3. No overlap:\n",
    "\n",
    "Candidate: ‚ÄúThis seat improves security in cars by providing a raised seat.‚Äù\n",
    "Reference: ‚ÄúFor the wall either concrete or wood could be used.‚Äù\n",
    "The sentences share no commonalities, thus neither BLEU, ROUGE, nor BERTScore should indicate a high level of similarity between them.\n",
    "\n",
    "I wrote the following code to calculate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lukas\\anaconda3\\envs\\tg\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_7200\\4124404111.py:70: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric('bleu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lukas\\anaconda3\\envs\\tg\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b_prec</th>\n",
       "      <th>b_rec</th>\n",
       "      <th>b_f1</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rouge1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.945100</td>\n",
       "      <td>0.922702</td>\n",
       "      <td>0.933767</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.889971</td>\n",
       "      <td>0.905214</td>\n",
       "      <td>0.897528</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.857613</td>\n",
       "      <td>0.831491</td>\n",
       "      <td>0.844350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     b_prec     b_rec      b_f1      bleu    rouge1\n",
       "0  0.945100  0.922702  0.933767  0.600000  0.666667\n",
       "1  0.889971  0.905214  0.897528  0.166667  0.380952\n",
       "2  0.857613  0.831491  0.844350  0.000000  0.000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "from statistics import mean\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "def calc_rouges(rouge_scores, rouge_type):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the average ROUGE score for a given ROUGE type from ROUGE scores.\n",
    "\n",
    "    Parameters:\n",
    "    - rouge_scores (dict): A dictionary containing ROUGE scores for different ROUGE types.\n",
    "    - rouge_type (str): The specific ROUGE type for which the average is calculated.\n",
    "\n",
    "    Returns:\n",
    "    - float: The average ROUGE score for the specified ROUGE type.\n",
    "    \"\"\"\n",
    "    # Extract ROUGE scores for high, mid, and low levels\n",
    "    rouge_h = rouge_scores[rouge_type].high.fmeasure\n",
    "    rouge_m = rouge_scores[rouge_type].mid.fmeasure\n",
    "    rouge_l = rouge_scores[rouge_type].low.fmeasure\n",
    "\n",
    "    # Calculate the average ROUGE score using the mean function\n",
    "    rouge_score = mean([rouge_h, rouge_m, rouge_l])\n",
    "\n",
    "    return rouge_score\n",
    "\n",
    "\n",
    "def make_predictions(sentence_pairs):\n",
    "\n",
    "    # Iterate through files in the '/processed' directory\n",
    "    for sentence_pair in sentence_pairs:\n",
    "            \n",
    "            candidate = sentence_pair['candidate']\n",
    "            ground1 = sentence_pair['ground1']\n",
    "            \n",
    "            # Compute BLEU score\n",
    "            bleu_scores = bleu_metric.compute(predictions=[candidate.split(' ')], references=[[ground1.split(' ')]])\n",
    "            bleu.append(bleu_scores['precisions'][0])\n",
    "\n",
    "            # Compute ROUGE score\n",
    "            \n",
    "            rouge_scores =  rouge_metric.compute(predictions=[candidate], references=[ground1])\n",
    "\n",
    "            for rouge_type,results in rouges.items():\n",
    "                rouges[rouge_type].append(calc_rouges(rouge_scores,rouge_type))\n",
    "\n",
    "            # Compute BERT_score\n",
    "            P, R, F1 = bertscore_metric.score([candidate], [ground1])\n",
    "            b_prec.append(P.item())\n",
    "            b_rec.append(R.item())\n",
    "            b_f1.append(F1.item())\n",
    "\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df_dict = {\n",
    "        \"b_prec\": b_prec,\n",
    "        \"b_rec\": b_rec,\n",
    "        \"b_f1\": b_f1,\n",
    "        \"bleu\": bleu,\n",
    "    }\n",
    "\n",
    "    df_dict.update(rouges)\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load BLEU and BERT_score metrics\n",
    "bleu_metric = load_metric('bleu')\n",
    "rouge_metric = load_metric('rouge')\n",
    "#Watch out: rescale_with_baseline=False the results change dramatically \n",
    "bertscore_metric = BERTScorer(lang=\"en\",rescale_with_baseline=False)\n",
    "\n",
    "# Initialize lists to store evaluation results\n",
    "patent_nos, bleu, b_prec, b_rec, b_f1 = [], [], [], [], []\n",
    "#For this we only need rouge1\n",
    "rouges = {\n",
    "    'rouge1':[],\n",
    "    #'rouge2':[],\n",
    "    #'rougeL':[],\n",
    "    }\n",
    "# Initialize dict for data\n",
    "sentence_pairs = [\n",
    "    {'candidate':'The method comprises receiving an input signal from a sensor.',\n",
    "       'ground1':'The method is about a sensor that recieves an input signal'},\n",
    "     {'candidate':'A computer is configured to communicate with a network for data transmission',\n",
    "       'ground1':'Data transfer is facilitated by a network-connectable laptop.'},\n",
    "    {'candidate':'This seat improves security in cars by providing a raised seat.',\n",
    "     'ground1':'For the wall either concrete or wood could be used'},\n",
    "]\n",
    "\n",
    "df = make_predictions(sentence_pairs)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence-Pair 1: While BERTScore indicates a strong similarity between the sentences, both BLEU and ROUGE also yield scores of 60% or higher.\n",
    "- Sentence Pair 2: Once more, BERTScore reveals a significant similarity, whereas BLEU and ROUGE suggest minimal resemblance.\n",
    "- Sentence Pair 3: As anticipated, neither BLEU nor ROUGE registers any resemblance, scoring 0%. But what is up with BERTScore? It scores for all three metrics well above 80 % even though the sentences have nothing in common.\n",
    "This mirrors the findings of my project. Yet, this raises questions about the reliability of BERTScore as a metric. To delve into this matter, I conducted a closer examination of BERTScore.\n",
    "\n",
    "## 4. The Pitfall of rescale_with_baseline = False\n",
    "In my experimentation with BERTScore, I noticed this peculiar behavior when rescale_with_baseline was set to False (this is the default setting for BERTScore). This parameter controls whether BERTScore rescales the final score with a baseline. Without this rescaling, the scores produced are skewed and inconsistent, leading to misleading evaluations of my LM‚Äôs performance.\n",
    "\n",
    "The crux of the matter lies in the necessity of setting rescale_with_baseline to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b_prec</th>\n",
       "      <th>b_rec</th>\n",
       "      <th>b_f1</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rouge1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.674175</td>\n",
       "      <td>0.541275</td>\n",
       "      <td>0.607564</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.346985</td>\n",
       "      <td>0.437490</td>\n",
       "      <td>0.392843</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154943</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.077762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     b_prec     b_rec      b_f1      bleu    rouge1\n",
       "0  0.674175  0.541275  0.607564  0.600000  0.666667\n",
       "1  0.346985  0.437490  0.392843  0.166667  0.380952\n",
       "2  0.154943 -0.000018  0.077762  0.000000  0.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertscore_metric = BERTScorer(lang=\"en\",rescale_with_baseline=True)\n",
    "\n",
    "# Initialize lists to store evaluation results\n",
    "patent_nos, bleu, b_prec, b_rec, b_f1 = [], [], [], [], []\n",
    "#For this we only need rouge1\n",
    "rouges = {\n",
    "    'rouge1':[],\n",
    "    #'rouge2':[],\n",
    "    #'rougeL':[],\n",
    "    }\n",
    "\n",
    "df = make_predictions(sentence_pairs)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon rescaling, the BERTScore values exhibit much more plausible results. They align closely with BLEU and ROUGE scores, notably revealing the anticipated differences in similarity among the sentence pairs.\n",
    "\n",
    "When enabled, BERTScore rescales the final score with a baseline similarity, ensuring that the scores are bounded within a reasonable range. This normalization mitigates the effects of varying token lengths and contextual differences between reference and candidate sentences, resulting in more reliable evaluations.\n",
    "\n",
    "## 5. Conclusion\n",
    "In the realm of language model evaluation, leveraging tools like BERTScore can significantly enhance the accuracy and reliability of assessments. However, it‚Äôs crucial to pay attention to the nuances of such metrics and understand the implications of their parameters. Through my journey, I‚Äôve come to appreciate the importance of setting rescale_with_baselineto Truefor accurate evaluations, ensuring that the assessments of language model performance are both meaningful and trustworthy.\n",
    "\n",
    "As the field of natural language processing continues to evolve, it‚Äôs imperative to stay informed about the latest methodologies and tools available for evaluation. BERTScore represents a significant step forward in this regard, offering a more nuanced and contextually aware approach to language model assessment. By understanding its intricacies and optimizing its parameters, we can unlock deeper insights into the capabilities and limitations of language models, ultimately driving innovation and progress in the field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
